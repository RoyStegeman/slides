\documentclass[aspectratio=169,11pt]{beamer}
\graphicspath{{figures/}} % Setting the graphicspath

% Theme settings
\usetheme{Madrid}
\usecolortheme{default}
\setbeamertemplate{navigation symbols}{}   % removes navigation symbols such as 'next page'
\setbeamertemplate{footline}{}             % remove line with name, date, page nr. 
\setbeamercolor*{frametitle}{bg=white}     % remove background from frametitle
\usepackage{caption}
% \captionsetup[figure]{labelformat=empty}% redefines the caption setup of the figures environment in the beamer class.
\setbeamersize{text margin left=20pt,text margin right=10pt}

\usefonttheme[onlymath]{serif} % makes beamer math look like article math


%======================= import packages =======================
\usepackage{pifont}       % Pi fonts (Digbats, symbol, etc.)
\usepackage{graphicx}     % More options for \includegraphics
\usepackage{appendixnumberbeamer} % separate appendix numbering
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{amsmath, nccmath}

%======================= define new commands =======================
\newcommand{\nn}{\vspace*{1em}}

%======================= page numbering =======================
\addtobeamertemplate{navigation symbols}{}{ \usebeamerfont{footline}
  \insertframenumber / \inserttotalframenumber \hspace*{2mm} \\ \vspace*{1mm} 
}


%=================================== colors ===================================
\definecolor{RoyBlue}{RGB}{22, 46, 69}
\definecolor{RoyGrey}{RGB}{64, 88, 128} 

\newcommand{\hlme}[1]{{\color{red}\bf #1}} % highlihgt me

\setbeamercolor{structure}{fg=RoyBlue} % itemize, enumerate, etc
\setbeamercolor{frametitle}{fg=RoyGrey}
\setbeamercolor{section in head/foot}{bg=RoyBlue}


%======================= add progress dots to headline =======================
\setbeamertemplate{headline}{%
    \begin{beamercolorbox}[ht=4mm,dp=4mm]{section in head/foot}
        \insertnavigation{\paperwidth}
    \end{beamercolorbox}%
}%
\makeatother


%======================= add section title page =======================
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
    \usebeamerfont{title}\insertsectionhead\par%
  \vfill
  \end{frame}
}


%=================================== titlepage ===================================
\title{Can the NNPDF architecture be simplified?}
\date{NNPDF Meeting, 6-8 September 2021, Gargnano}
\author{Roy Stegeman}
\institute{University of Milan and INFN Milan}
\titlegraphic{\vspace*{6mm}
  \includegraphics[height=0.8cm]{logos/LOGO-ERC.jpg} \hspace{10mm}
	\includegraphics[height=0.8cm]{logos/n3pdflogo_noback.png} \hspace{10mm}
	\includegraphics[height=0.6cm]{logos/nnpdf_logo_official.pdf} \hspace{10mm}
	\includegraphics[height=0.8cm]{logos/Logo_Università_degli_Studi_di_Milano(not_mandatory).png}
	\includegraphics[height=0.8cm]{logos/INFN_logo.png}
  \vspace*{5mm} \\
	\centering{ 
    \fontsize{7.0pt}{0.0pt}\selectfont This project has received funding from the European Union’s Horizon 2020 \\	
    \vspace*{-1mm}
    research and innovation programme under grant agreement No 740006.
	}
}


\begin{document}
{
\setbeamertemplate{headline}{} % remove headline from titlepage
\begin{frame}
  \titlepage
\end{frame}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{The NNPDF model}



\begin{frame}[t]{The NNPDF model}
  \only<1> {$$ xf_k(x) = A_k x^{1-\alpha_k}(1-x)^{\beta_k} \mathrm{NN}_k(x,\log x) $$}
  \only<2-> {{\bf Preprocessing} exists for historic reasons, it is still used to improve convergence and to model the extrapolation region. \\ \nn}
  \only<3-> {$(x,\log x)$ mixes objects with different orders of magnitude, which is poor practice from an ML standpoint \\ \nn}
  \only<2> {$$ xf_k(x) = A_k {\color{red} x^{1-\alpha_k}(1-x)^{\beta_k}} \mathrm{NN}_k(x,\log x) $$}
  \only<3-> {$$ xf_k(x) = A_k {\color{red} x^{1-\alpha_k}(1-x)^{\beta_k}} \mathrm{NN}_k({\color{red} x,\log x}) $$}
  \only<4> {\\ \vspace*{1.5cm} \bf \centering Can we simplify this architecture?}
\end{frame}


\begin{frame}[t]{A naive attempt}
  An obvious solution might be to replace $(x,\log x)$ with $(x)$, but this results in saturation of the activation functions:
  \begin{center}
    \includegraphics[height=0.6\textheight]{pdf_sbar_log_saturated.pdf} 
    \includegraphics[height=0.6\textheight]{pdf_u_log_saturated.pdf}
    % \\ \nn {\bf What has happened here?}
  \end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Input scaling}


\begin{frame}[t]{Input scaling: The problem}
  Starting from the FK-tables xgrid distribution:\\
  \begin{center}
    \includegraphics[height=0.4\textheight]{figures/default_xgrid.png}
  \end{center}
  The optimizer is not sensitive for input across multiple orders of magnitude, and without any scaling will only be able to fit features on a linear scale. \\ \nn
  The $(x,\log x)$ input scaling makes the optimizer sensitive to input on a logarithmic scale as well \\ \nn
  We can do better!
\end{frame}


\begin{frame}[t]{Input scaling: The idea}
  Let us take a page from the ML community's playbook.\\
  Equalize the histogram using the empirical cumulative distribution function (CDF):
  \begin{center}
    \includegraphics[height=0.4\textheight]{figures/default_xgrid.png} \hspace*{1cm}
    \includegraphics[height=0.4\textheight]{figures/ecdf_xgrid.png}
  \end{center}
  Now data is no longer distributed across many orders of magnitude \\ \nn
  Add an interpolation function and we're done!
\end{frame}


\begin{frame}[t]{Result}
  There appear to be no saturation in the data region:\\
  \begin{center}
    \includegraphics[height=0.5\textheight]{figures/pdf_sbar_log_feature_vs_nnpdf40.pdf}
    \includegraphics[height=0.5\textheight]{pdf_u_log_feature_vs_nnpdf40.pdf}
  \end{center}
  The results shown in this talk remove both preprocessing and $(x,\log x)$, but they should be thought of as independent ideas.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Removing preprocessing}


\begin{frame}[t]{Obstacles to removing preprocessing}
  Getting rid of preprocessing simplifies the model architecture and results in an increased stability between replicas. \\ \nn
  A specific example where this can be beneficial is hyperoptimization (see Juan CM's talk) \\ \vspace*{1.2cm}

  The suggestion of removing preprocessing raises two obvious questions:
  \begin{enumerate}
    \item Can it be done without ruining convergence?
    \item Can the extrapolation region still be trusted?
  \end{enumerate}
  \nn
  \begin{center}
    Let's {\color{blue} \underline{\href{https://vp.nnpdf.science/TVyAUeiNTk26IMYAfRNqLw==}{compare}}} `feature scaling' to NNPDF4.0
  \end{center}
\end{frame}


\begin{frame}[t]{Convergence}
  Statistics are unchanged:\\
  \begin{center}
    \includegraphics[height=0.5\textheight]{figures/summary_feature_vs_nnpdf40.png}
  \end{center}
\end{frame}


\begin{frame}[t]{Convergence}
  Remember that the PDFs are similar:\\
  \begin{center}
    \includegraphics[height=0.5\textheight]{figures/pdf_sbar_log_feature_vs_nnpdf40.pdf}
    \includegraphics[height=0.5\textheight]{figures/pdf_u_log_feature_vs_nnpdf40.pdf}
  \end{center}
\end{frame}


\begin{frame}[t]{The extrapolation region}
  Uncertainty in the extrapolation region is the same:\\
  \begin{center}
    \includegraphics[height=0.5\textheight]{figures/pdf_extra_sbar_feature_vs_nnpdf40.pdf}
    \includegraphics[height=0.5\textheight]{figures/pdf_extra_u_feature_vs_nnpdf40.pdf}
  \end{center}
  Note the plotted range of $x$, LHAPDF grids are provided for $x\geq 10^{-9}$
\end{frame}

\begin{frame}[t]{Future test}
  Let's compare the future tests of {\color{blue} \underline{\href{https://vp.nnpdf.science/TVyAUeiNTk26IMYAfRNqLw==}{feature scaling}}} and {\color{blue} \underline{\href{https://vp.nnpdf.science/ArCroD6xRxmTAGHSBJoD3Q==/}{NNPDF4.0}}}
  \begin{center}
    \begin{figure}
      \minipage{0.49\textwidth}
        \includegraphics[width=1\textwidth]{figures/futuretest_nnpdf40.png}
        \captionsetup{labelformat=empty}
        \caption{NNPDF4.0}
      \endminipage\hfill
      \minipage{0.49\textwidth}
        \includegraphics[width=1\textwidth]{figures/futuretest_feature.png}
        \captionsetup{labelformat=empty}
        \caption{feature scaling}
      \endminipage
    \end{figure}
    \vspace*{1em}
    \only<2> {The future test is successful}
  \end{center}
\end{frame}


\begin{frame}[t]{Towards phenomenology: luminosities}
  Good agreement at the level of luminosities:
  \begin{center}
    \includegraphics[height=0.5\textheight]{figures/1dlumi_gg_feature_vs_nnpdf40.pdf}
    \includegraphics[height=0.5\textheight]{figures/1dlumi_qq_feature_vs_nnpdf40.pdf}
  \end{center}
\end{frame}


\begin{frame}[t]{Phenomenology}{Thanks Christopher for these {\color{blue} \underline{\href{https://vp.nnpdf.science/de96i8VBQ9Gc-zxl75AMfw==/pheno_featurescaling.pdf}{plots}}}}
  Most importantly - good agreement at the level of observables:
  \begin{center}
    \includegraphics[width=0.45\textwidth]{figures/pheno_w.png} \hfill
    \includegraphics[width=0.45\textwidth]{figures/pheno_z.png}
  \end{center}
  Feature scaling consistency results in larger uncertainties, while other measures do not deteriorate
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cartomancy: Predicting the extrapolation region}


\begin{frame}[t]{Modelling experimental data}
  Based on an idea proposed during the NNPDF meeting in Amsterdam 2020: see 
  {\color{blue} \underline{\href{https://www.wiki.ed.ac.uk/download/attachments/432523942/carrazza.pdf?version=1&modificationDate=1581344104000&api=v2}{slides}}} \\ \nn
  The proposed small-$x$ strategy is as follows:
  \begin{enumerate}
    \item Select DIS datasets with small-$x$ points
    \item Build a Gaussian Process (GP) model which learns the DIS dataset
    \item Create an xgrid in the small-$x$ extrapolation region
    \item Generate pseudo-data at small-$x$ using the GP model
    \item Compute FK tables for those points
  \end{enumerate} 
\end{frame}



\begin{frame}[t]{Modelling experimental data: Example}
  \begin{center}
    \includegraphics[height=0.6\textheight]{figures/gp_hera.png}
  \end{center}
  Observe that the uncertainty of the psuedo-data is much larger than of the experimental data
\end{frame}


\begin{frame}[t]{Fitting psuedo-data}
  With `help' (fitted preprocessing or increased weight) the psuedo-data can be fitted:
  \begin{center}
    \includegraphics[height=0.5\textheight]{figures/extradata_fitted.pdf}
  \end{center}
  But, even if there is no friction with other datasets, without such help the uncertainties are too large and other datasets dominate the $\chi^2$
\end{frame}


\begin{frame}[t]{Problems}
  \begin{enumerate}
    \item Is the prediction accurate (choice of kernel and parameters)?
    \begin{itemize}
      \item `Future test' the kernels
    \end{itemize}
    \item Is there a way to fit data with large uncertainties?
    \begin{itemize}
      \item Perhaps something along the line of Zahari's ensemble of fits with weighed datasets
    \end{itemize}
  \end{enumerate}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bgroup
  \setbeamercolor{background canvas}{bg=RoyBlue}
  \setbeamertemplate{navigation symbols}{}
  \begin{frame}[plain,noframenumbering]{}
    \color{white}
    \huge
    \begin{center}
      \textbf{Thank you!}
    \end{center}
  \end{frame}
\egroup


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \input{backup.tex}

\end{document}
